У машинному навчанні штучна нейронна мережа (також нейронна мережа або нейронна мережа, скорочено ANN або NN ) — це модель, натхненна нейронною організацією, знайденою в біологічних нейронних мережах у мозку тварин.

Штучна нейронна мережа — це взаємопов’язана група вузлів, натхненна спрощенням нейронів у мозку . Тут кожен круглий вузол позначає штучний нейрон , а стрілка позначає з’єднання від виходу одного штучного нейрона до входу іншого.
ШНМ складається із з’єднаних одиниць або вузлів, які називаються штучними нейронами , які нещільно моделюють нейрони в мозку. Вони з’єднані краями , які моделюють синапси в мозку. Штучний нейрон отримує сигнали від підключених нейронів, потім обробляє їх і посилає сигнал іншим підключеним нейронам. «Сигнал» — це дійсне число , а вихід кожного нейрона обчислюється деякою нелінійною функцією суми його входів, яка називається функцією активації . Нейрони та ребра зазвичай мають вагу , яка коригується в міру навчання. Вага збільшує або зменшує силу сигналу під час підключення.

Як правило, нейрони агреговані в шари. Різні шари можуть виконувати різні перетворення своїх вхідних даних. Сигнали проходять від першого шару ( вхідного шару ) до останнього ( вихідного шару ), можливо, проходячи через кілька проміжних шарів ( прихованих шарів ). Мережа зазвичай називається глибокою нейронною мережею, якщо вона має принаймні 2 прихованих шари. [3]

Штучні нейронні мережі використовуються для прогнозного моделювання, адаптивного керування та інших програм, де їх можна навчати за допомогою набору даних. Вони також використовуються для вирішення проблем штучного інтелекту . Мережі можуть вчитися на досвіді та робити висновки зі складного та, здавалося б, непов’язаного набору інформації.

Нейронні мережі зазвичай навчаються шляхом емпіричної мінімізації ризику. Цей метод базується на ідеї оптимізації параметрів мережі для мінімізації різниці або емпіричного ризику між прогнозованим результатом і фактичними цільовими значеннями в даному наборі даних. [4] Для оцінки параметрів мережі зазвичай використовуються градієнтні методи, такі як зворотне поширення . [4] Під час фази навчання ШНМ навчаються з помічених навчальних даних шляхом повторного оновлення своїх параметрів, щоб мінімізувати визначену функцію втрат . [5] Цей метод дозволяє мережі узагальнювати невидимі дані.

Спрощений приклад навчання нейронної мережі виявленню об’єктів: мережа навчається кількома зображеннями, які, як відомо, зображують морські зірки та морських їжаків , які співвідносяться з «вузлами», що представляють візуальні особливості . Морські зірки поєднуються з кільцевою текстурою та зірковим контуром, тоді як більшість морських їжаків мають смугасту текстуру та овальну форму. Однак екземпляр морського їжака з кільцевою текстурою створює слабкий зв’язок між ними.

Подальший запуск мережі на вхідному зображенні (зліва): Мережа правильно визначає морську зірку. Однак слабкий зв’язок між кільчастою текстурою та морським їжаком також надає останньому слабкий сигнал від одного з двох проміжних вузлів. Крім того, черепашка, яка не була включена в навчання, дає слабкий сигнал для овальної форми, що також призводить до слабкого сигналу для виведення морського їжака. Ці слабкі сигнали можуть призвести до хибнопозитивного результату для морського їжака.
Насправді текстури та контури будуть представлені не окремими вузлами, а скоріше асоційованими шаблонами ваги кількох вузлів.

Історично склалося так, що цифрові комп’ютери еволюціонували з моделі фон Неймана і працюють через виконання явних інструкцій через доступ до пам’яті кількома процесорами. З іншого боку, нейронні мережі виникли в результаті спроб змоделювати обробку інформації в біологічних системах за допомогою коннекціонізму . На відміну від моделі фон Неймана, коннекціоністські обчислення не розділяють пам’ять і обробку.

Найпростішим типом прямої нейронної мережі (FNN) є лінійна мережа, яка складається з одного шару вихідних вузлів; входи подаються безпосередньо на виходи через серію ваг. Сума добутків ваг і вхідних даних обчислюється в кожному вузлі. Середньоквадратичні похибки між цими обчисленими виходами та заданими цільовими значеннями мінімізуються шляхом створення коригування ваг. Цей метод відомий понад два століття як метод найменших квадратів або лінійна регресія . Лежандр (1805) і Гаусс (1795) використовували його як засіб для знаходження хорошої грубої лінійної відповідності набору точок для передбачення руху планет. [7] [8] [9] [10] [11]

Уоррен МакКаллох і Уолтер Піттс [12] (1943) також розглядали ненавчальну обчислювальну модель для нейронних мереж. [13]

Наприкінці 1940-х років Д. О. Хебб [14] створив гіпотезу навчання , засновану на механізмі нервової пластичності , яка стала відомою як навчання Хебба . Геббівське навчання вважається «типовим» правилом неконтрольованого навчання , а його пізніші варіанти були ранніми моделями довгострокового потенціювання . Ці ідеї почали застосовувати до обчислювальних моделей у 1948 році з « неорганізованими машинами » Тюрінга. Фарлі та Уеслі А. Кларк [15] першими змоделювали мережу Хебба в 1954 році в MIT. Вони використовували обчислювальні машини, які тоді називали «калькуляторами». Інші обчислювальні машини з нейронними мережами були створені Рочестером, Голландом, Хабітом і Дудою [16] у 1956 році. У 1958 році психолог Френк Розенблатт винайшов перцептрон , першу реалізовану штучну нейронну мережу [17] [18] [19] [20]. ] фінансується Управлінням військово-морських досліджень США . [21]

Винахід персептрона підняв суспільний інтерес до досліджень штучних нейронних мереж, що змусило уряд США різко збільшити фінансування досліджень глибокого навчання. Це призвело до «золотого віку штучного інтелекту», викликаного оптимістичними твердженнями комп’ютерників щодо здатності перцептронів імітувати людський інтелект. [22] Наприклад, у 1957 році Герберт Саймон сказав знамените: [22]
Я не маю на меті здивувати чи шокувати вас, але найпростіший спосіб підсумувати це сказати, що зараз у світі існують машини, які думають, навчаються і творять. Більше того, їхня здатність робити ці речі швидко зростатиме, доки — у майбутньому — коло проблем, які вони можуть вирішити, не зрівняється з діапазоном, до якого застосовувався людський розум.

Однак це було не так, оскільки дослідження в Сполучених Штатах застопорилися після роботи Мінскі та Пейперта (1969), [23] які виявили, що базові персептрони не здатні обробляти схему виключного або, а комп’ютери не мають достатньої потужності для тренувати корисні нейронні мережі. Це разом з іншими факторами, такими як звіт Лайтхілла 1973 року Джеймса Лайтхілла, в якому зазначено, що дослідження в області штучного інтелекту не «забезпечили значного впливу, який тоді обіцяли», припиняючи фінансування досліджень у галузі штучного інтелекту в усіх університетах, крім двох, Великобританії та в багатьох великих установах по всьому світу. [24] Це започаткувало епоху під назвою « Зимовий штучний інтелект» із скороченням досліджень коннекціонізму через зменшення державного фінансування та посилення наголосу на символічний штучний інтелект у Сполучених Штатах та інших західних країнах. [25] [24]

Однак під час зимової ери ШІ дослідження за межами Сполучених Штатів тривали, особливо у Східній Європі. На той час , коли вийшла книга Мінських і Пеперта про персептрони , методи навчання багатошарових персептронів (MLP) вже були відомі. Перший MLP глибокого навчання був опублікований Олексієм Григоровичем Івахненком і Валентином Лапою в 1965 році під назвою Груповий метод обробки даних . [26] [27] [28] Перший MLP глибокого навчання, навчений стохастичним градієнтним спуском [29], був опублікований у 1967 році Шунічі Амарі. [30] [31] У комп’ютерних експериментах, проведених учнем Амарі Сайто, п’ятишаровий MLP з двома модифікованими шарами навчився корисним внутрішнім представленням для класифікації нелінійно роздільних класів шаблонів. [31]

Самоорганізуючі карти (SOM) були описані Теуво Кохоненом у 1982 році. [32] [33] SOM — це нейрофізіологічні [34] нейронні мережі, які вивчають низьковимірні представлення високовимірних даних, зберігаючи при цьому топологічну структуру даних. Вони навчаються за допомогою конкурентного навчання . [32]

Архітектура згорткової нейронної мережі (CNN) із згортковими шарами та шарами зменшення дискретизації була представлена ​​Куніхіко Фукусімою в 1980 році . [35] Він назвав її неокогнітроном . У 1969 році він також представив функцію активації ReLU (випрямлена лінійна одиниця) . [36] [10] Випрямляч став найпопулярнішою функцією активації для CNN і глибоких нейронних мереж загалом. [37] CNN стали важливим інструментом комп’ютерного зору .

Ключовим у подальших досягненнях у дослідженні штучних нейронних мереж був алгоритм зворотного поширення , ефективне застосування правила ланцюга Лейбніца (1673) [38] до мереж диференційованих вузлів. [10] Він також відомий як зворотний режим автоматичної диференціації або зворотного накопичення , завдяки Сеппо Ліннайнмаа (1970). [39] [40] [41] [42] [10] Термін «помилки зворотного поширення» був введений у 1962 році Френком Розенблатом [43] [10] , але він не мав реалізації цієї процедури, хоча Генрі Дж. Келлі [44] і Брайсон [45] мали безперервні попередники зворотного поширення на основі динамічного програмування [26] [46] [47] [48] вже в 1960–61 роках у контексті теорії керування . [10] У 1973 році Дрейфус використав зворотне поширення для адаптації параметрів контролерів пропорційно градієнтам помилок. [49] У 1982 році Пол Вербос застосував зворотне поширення до MLP у спосіб, який став стандартним. [50] [46] У 1986 році Румельхарт , Хінтон і Вільямс показали, що зворотне розповсюдження вивчає цікаві внутрішні представлення слів як векторів ознак, коли навчається передбачати наступне слово в послідовності. [51]

Наприкінці 1970-х – на початку 1980-х років ненадовго з’явився інтерес до теоретичного дослідження моделі Ізінга , створеної Вільгельмом Ленцом (1920) та Ернстом Ізінгом (1925) [52] у зв’язку з топологією дерева Кейлі та великими нейронними мережами . Модель Ізінга, по суті, є штучною рекурентною нейронною мережею (RNN) , що не навчається, і складається з нейроноподібних порогових елементів. [10] У 1972 році Шунічі Амарі описав адаптивну версію цієї архітектури [53] [10] У 1981 році модель Ізінга була точно розв'язана Пітером Бартом для загального випадку закритих дерев Кейлі (з петлями) з довільне відношення розгалуження [54] , і виявлено, що він демонструє незвичайну поведінку фазового переходу в його локальній вершині та дальній кореляції сайт-сайт. [55] [56] Джон Хопфілд популяризував цю архітектуру в 1982 році, [57] і зараз вона відома як мережа Хопфілда .

Нейронна мережа із затримкою часу (TDNN) Алекса Вайбеля (1987) об’єднала згортки, розподіл ваги та зворотне поширення. [58] [59] У 1988 році Wei Zhang et al. застосував зворотне розповсюдження до CNN (спрощений Neocognitron зі згортковими взаємозв’язками між шарами ознак зображення та останнім повністю зв’язаним шаром) для розпізнавання алфавіту. [60] [61] У 1989 році Yann LeCun та ін. навчив CNN розпізнавати рукописні поштові індекси на пошті. [62] У 1992 році Хуан Венг та ін. запровадили максимальне об’єднання для CNN. щоб допомогти з інваріантністю найменшого зсуву та толерантністю до деформації для сприяння розпізнаванню 3D-об’єктів . [63] [64] [65] LeNet-5 (1998), 7-рівневий CNN від Yann LeCun та ін., [66] який класифікує цифри, був застосований кількома банками для розпізнавання рукописних чисел на чеках, оцифрованих у Зображення 32х32 пікселя.

Починаючи з 1988 року [67] [68] використання нейронних мереж трансформувало сферу передбачення структури білків , зокрема, коли перші каскадні мережі навчалися на профілях (матрицях), створених численними вирівнюваннями послідовностей . [69]

У 1991 році в дипломній роботі Зеппа Гохрейтера [70] була визначена та проаналізована проблема зникнення градієнта [70] [71] і запропоновані рекурентні залишкові зв'язки для її вирішення. Його науковий керівник Юрген Шмідхубер назвав його дисертацію «одним із найважливіших документів в історії машинного навчання» . [10]

У 1991 році Юрген Шмідхубер опублікував змагальні нейронні мережі, які змагаються одна з одною у формі гри з нульовою сумою , де виграш однієї мережі є програшем іншої мережі. [72] [73] [74] Перша мережа є генеративною моделлю , яка моделює розподіл ймовірностей за моделями вихідних даних. Друга мережа вчиться шляхом градієнтного спуску передбачати реакцію середовища на ці шаблони. Це було названо «штучною цікавістю».

У 1992 році Юрген Шмідгубер запропонував ієрархію RNN, попередньо навчених по одному рівню за допомогою самонавчання . [75] Він використовує прогнозне кодування для вивчення внутрішніх представлень у кількох самоорганізованих масштабах часу. Це може істотно полегшити подальше глибоке навчання. Ієрархію RNN можна згорнути в єдину RNN шляхом дистиляції мережі блокування вищого рівня в мережу автоматизатора нижчого рівня . [75] [10] У тому ж році він також опублікував альтернативу RNN [76] , яка є попередником лінійного трансформатора . [77] [78] [10] Він вводить концепцію внутрішніх прожекторів уваги : ​​[79] повільна нейронна мережа прямого зв’язку вчиться шляхом градієнтного спуску контролювати швидкі ваги іншої нейронної мережі через зовнішні продукти самогенерованих шаблонів активації.

Розвиток дуже великомасштабної інтеграції (VLSI ) метал-оксид-напівпровідник (MOS ) у формі додаткової технології MOS (CMOS) дозволив збільшити кількість MOS-транзисторів у цифровій електроніці . Це забезпечило більшу обчислювальну потужність для розробки практичних штучних нейронних мереж у 1980-х роках. [80]

Ранні успіхи нейронних мереж включали прогнозування фондового ринку, а в 1995 році (переважно) безпілотний автомобіль. [a] [81]

У 1997 році Зепп Хохрайте та Юрген Шмідхубер представили метод глибокого навчання під назвою довготривала короткочасна пам’ять (LSTM), опублікований у Neural Computation. [82] Рекурентні нейронні мережі LSTM можуть вивчати завдання «дуже глибокого навчання» [83] з довгими шляхами розподілу кредитів, які потребують спогадів про події, які відбулися тисячі окремих часових кроків раніше. «Vanilla LSTM» із забутими воротами був представлений у 1999 році Феліксом Герсом , Шмідгубером та Фредом Каммінсом. [84]

Джеффрі Хінтон та ін. (2006) запропонував вивчати представлення високого рівня з використанням послідовних шарів двійкових або реальних латентних змінних з обмеженою машиною Больцмана [85] для моделювання кожного шару. У 2012 році Нг і Дін створили мережу, яка навчилася розпізнавати концепції вищого рівня, такі як коти, лише дивлячись зображення без позначок. [86] Неконтрольоване попереднє навчання та збільшення обчислювальної потужності графічних процесорів і розподілених обчислень дозволили використовувати більші мережі, зокрема в проблемах розпізнавання зображень і візуального зображення, які стали відомі як «глибоке навчання». [5]

Варіанти алгоритму зворотного поширення , а також самостійні методи Джеффа Хінтона та його колег з Університету Торонто можуть бути використані для навчання глибоких, дуже нелінійних нейронних архітектур, [87] подібних до Неокогнітрона 1980 року Куніхіко Фукусіми [88] . ] і «стандартна архітектура зору», [89] натхненна простими і складними клітинами, визначеними Девідом Х. Хьюбелом і Торстеном Візелем у первинній зоровій корі .

Обчислювальні пристрої були створені в CMOS як для біофізичного моделювання, так і для нейроморфних обчислень . Недавні зусилля показують багатообіцяючі перспективи створення нанопристроїв для дуже великомасштабного аналізу головних компонентів і згортки . [90] У разі успіху ці зусилля можуть започаткувати нову еру нейронних обчислень , які є кроком за межі цифрових обчислень [91] , оскільки вони залежать від навчання , а не від програмування , і тому, що вони за своєю суттю аналогові , а не цифрові , навіть незважаючи на те, що перші екземпляри насправді може бути з цифровими пристроями CMOS.

Ciresan та його колеги (2010) [92] показали, що, незважаючи на проблему зникаючого градієнта , графічні процесори роблять зворотне поширення можливим для багаторівневих нейронних мереж прямого зв’язку. [93] Між 2009 і 2012 роками ШНМ почали вигравати нагороди в конкурсах з розпізнавання зображень, наближаючись до людського рівня виконання різних завдань, спочатку в розпізнаванні образів і розпізнаванні рукописного тексту . [94] [95] Наприклад, двонаправлена ​​та багатовимірна довготривала короткочасна пам'ять (LSTM) [96] [97] Грейвса та ін. виграв три змагання з розпізнавання пов’язаного рукописного тексту в 2009 році без будь-яких попередніх знань про три мови, які потрібно вивчати. [96] [97]

Сіресан і його колеги створили перші пристрої розпізнавання шаблонів для досягнення конкурентоспроможних/надлюдських характеристик [98] на таких тестах, як розпізнавання дорожніх знаків (IJCNN 2012).

Радіальна базисна функція та вейвлет-мережі були представлені в 2013 році. Можна продемонструвати, що вони пропонують найкращі властивості апроксимації та були застосовані в програмах ідентифікації та класифікації нелінійних систем . [99]

У 2014 році принцип змагальності був використаний у генеративній змагальній мережі (GAN) Ian Goodfellow та ін. [100] Тут змагальна мережа (дискримінатор) виводить значення від 1 до 0 залежно від імовірності того, що вихід першої мережі (генератора) є в даному наборі. Це можна використовувати для створення реалістичних дипфейків . [101] Чудова якість зображення досягається завдяки StyleGAN ( 2018) Nvidia [102] на основі прогресивного GAN від Теро Карраса, Тімо Айли, Самулі Лайне та Яакко Лехтінена. [103] Тут генератор GAN вирощується від малого до великого у пірамідальний спосіб.

У 2015 році Рупеш Кумар Шрівастава, Клаус Грефф і Шмідхубер використали принцип LSTM для створення мережі Highway , прямої нейронної мережі із сотнями шарів, набагато глибшої, ніж попередні мережі. [104] [105] 7 місяців потому, Каймін Хе, Сян'ю Чжан; Shaoqing Ren і Jian Sun виграли конкурс ImageNet 2015 із варіантом мережі Highway із відкритим або без шлюзів під назвою Residual neural network . [106]

У 2017 році Ashish Vaswani та ін. представили сучасну архітектуру Transformer у своїй статті «Увага — це все, що вам потрібно». [107] Він поєднує це з оператором softmax і матрицею проекції. [10] Трансформатори все частіше стають моделлю вибору для обробки природної мови . [108] Багато сучасних великих мовних моделей, таких як ChatGPT , GPT-4 і BERT , використовують його. Трансформатори також все частіше використовуються в комп'ютерному зорі . [109]

Рамензанпур та ін. показали у 2020 році, що аналітичні та обчислювальні методи, отримані зі статистичної фізики невпорядкованих систем, можна поширити на масштабні проблеми, включаючи машинне навчання, наприклад, для аналізу вагового простору глибоких нейронних мереж. [110]

ШНМ перетворилися на широке сімейство методів, які вдосконалили сучасні технології в багатьох областях. Найпростіші типи мають один або кілька статичних компонентів, включаючи кількість одиниць, кількість шарів, вагу одиниць і топологію . Динамічні типи дозволяють одному або декільком із них розвиватися шляхом навчання. Останній є набагато складнішим, але може скоротити періоди навчання та дати кращі результати. Деякі типи дозволяють/вимагають «контролю» навчання оператором, тоді як інші працюють незалежно. Деякі типи працюють виключно апаратно, тоді як інші є суто програмними і працюють на комп’ютерах загального призначення.

Деякі з основних проривів включають:

Згорткові нейронні мережі , які виявилися особливо успішними в обробці візуальних та інших двовимірних даних; [151] [152] де довготривала короткочасна пам'ять уникає проблеми градієнта, що зникає [153], і може обробляти сигнали, які мають поєднання низькочастотних і високочастотних компонентів, що сприяє розпізнаванню мовлення з великим словниковим запасом, [154] [155] перетворення тексту -синтез мовлення, [156] [46] [157] та фотореальні розмовляючі голови; [158]
Конкурентні мережі, такі як генеративні змагальні мережі , в яких численні мережі (з різною структурою) змагаються одна з одною за такі завдання, як перемога в грі [159] або за введення в оману опонента щодо автентичності вхідних даних. [100]

Завдяки своїй здатності відтворювати та моделювати нелінійні процеси, штучні нейронні мережі знайшли застосування в багатьох дисциплінах. До них належать:

Функціональна апроксимація [ 164] або регресійний аналіз [165] ( включаючи прогнозування часових рядів , апроксимацію відповідності [166] та моделювання)
Обробка даних [167] (включаючи фільтрацію, кластеризацію, сліпе розділення джерел [ 168] і стиснення)
Ідентифікація нелінійної системи [99] та керування (включаючи керування транспортним засобом, прогнозування траєкторії, [169] адаптивне керування , керування процесами та управління природними ресурсами )
Розпізнавання образів (включаючи радарні системи, ідентифікацію обличчя , класифікацію сигналів, [170] виявлення новизни , 3D-реконструкцію , [171] розпізнавання об'єктів і послідовне прийняття рішень [172] )
Розпізнавання послідовності (включаючи розпізнавання жестів , мови , рукописного та друкованого тексту [173] )
Аналіз даних сенсора [174] (включаючи аналіз зображення )
Робототехніка (включаючи керівні маніпулятори та протези )
Інтелектуальний аналіз даних (включаючи виявлення знань у базах даних )
Фінанси [175] (такі як прогнозовані моделі для конкретних фінансових довгострокових прогнозів і штучних фінансових ринків )
Квантова хімія [176]
Загальна гра [177]
Генеративний ШІ [178]
Візуалізація даних
Машинний переклад
Фільтр соціальних мереж [179]
Фільтрація спаму в електронній пошті
Медичний діагноз
ШНМ використовувалися для діагностики кількох типів раку [180] [181] і для відрізнення високоінвазивних ліній ракових клітин від менш інвазивних, використовуючи лише інформацію про форму клітин. [182] [183]

ШМН використовувалися для прискорення аналізу надійності інфраструктури, що піддається стихійним лихам [184] [185] , і для прогнозування осідання фундаменту. [186] Це також може бути корисним для пом’якшення повені шляхом використання ШНМ для моделювання дощового стоку. [187] ШНМ також використовувалися для створення моделей чорної скриньки в геонауках : гідрологія , [188] [189] моделювання океану та берегова інженерія [190] [191] та геоморфологія. [192] ШМН використовуються в кібербезпеці з метою розмежування законної діяльності від зловмисної. Наприклад, машинне навчання використовувалося для класифікації зловмисного програмного забезпечення Android [193] для ідентифікації доменів, що належать суб’єктам загрози, і для виявлення URL-адрес, які становлять загрозу безпеці. [194] Ведуться дослідження систем ANN, розроблених для тестування на проникнення, для виявлення ботнетів, [195] шахрайства з кредитними картками [196] та мережевих вторгнень.

ШНМ були запропоновані як інструмент для розв’язування диференціальних рівнянь з частинними похідними у фізиці [197] [198] [199] і моделювання властивостей відкритих квантових систем багатьох частинок . [200] [201] [202] [203] У дослідженні мозку ШНМ вивчали короткочасну поведінку окремих нейронів , [204] динаміка нейронних схем виникає через взаємодію між окремими нейронами та те, як поведінка може виникати з абстрактних нейронних модулів, які являють собою завершені підсистеми. Дослідження розглядали довгострокову та короткочасну пластичність нейронних систем та їх зв’язок із навчанням і пам’яттю від індивідуального нейрона до системного рівня.

З картинок можна створити профіль інтересів користувача, використовуючи штучні нейронні мережі, навчені розпізнаванню об’єктів. [205]

Окрім традиційних застосувань, штучні нейронні мережі все частіше використовуються в міждисциплінарних дослідженнях, таких як матеріалознавство. Наприклад, графові нейронні мережі (GNN) продемонстрували свою здатність масштабувати глибинне навчання для відкриття нових стабільних матеріалів шляхом ефективного прогнозування загальної енергії кристалів. Ця програма підкреслює адаптивність і потенціал ШНМ у вирішенні складних проблем, що виходять за рамки прогнозного моделювання та штучного інтелекту, відкриваючи нові шляхи для наукових відкриттів та інновацій. [206]