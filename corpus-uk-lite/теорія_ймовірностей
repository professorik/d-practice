Теорія ймовірностей — це розділ математики, що стосується подій і числових описів їхньої ймовірності. Імовірність події – це число від 0 до 1; чим більша ймовірність, тим більший щанс настання події. Простим прикладом є підкидання чесної (неупередженої) монети. Оскільки монета справедлива, обидва результати («орел» і «решка») однаково ймовірні; ймовірність «голів» дорівнює ймовірності «хвостів»; а оскільки інші результати неможливі, ймовірність «орел» або «хвостів» становить 1/2 (що також можна записати як 0,5 або 50%).

Імовірності викидання кількох чисел за допомогою двох кубиків
Ці концепції отримали аксіоматичну математичну формалізацію в теорії ймовірностей, яка широко використовується в таких галузях дослідження, як статистика, математика, наука, фінанси, азартні ігри, штучний інтелект, машинне навчання, інформатика, теорія ігор і філософія для наприклад, зробити висновок про очікувану частоту подій. Теорія ймовірностей також використовується для опису основної механіки та закономірностей складних систем.

Коли ми маємо справу з випадковими експериментами , тобто експериментами , які є випадковими та чітко визначеними , у суто теоретичних умовах (наприклад, підкидання монети), ймовірності можна описати чисельно кількістю бажаних результатів, поділеним на загальну кількість усіх результатів. Це називається теоретичною ймовірністю (на відміну від емпіричної ймовірності, яка має справу з ймовірностями в контексті реальних експериментів). Наприклад, двічі підкинувши монету, ви отримаєте результати «хед-хед», «хед-хед», «хед-хед», «хех-хед» і «хед-хех». Імовірність отримання результату «голова-голова» становить 1 з 4 результатів, або, в цифровому вираженні, 1/4, 0,25 або 25%. Однак, коли справа доходить до практичного застосування, існують дві основні конкуруючі категорії інтерпретацій ймовірності, прихильники яких дотримуються різних поглядів на фундаментальну природу ймовірності:

Об'єктивісти призначають числа для опису певного об'єктивного або фізичного стану речей. Найбільш популярною версією об’єктивної ймовірності є частотна ймовірність , яка стверджує, що ймовірність випадкової події означає відносну частоту появи результату експерименту, коли експеримент повторюється нескінченно довго. Ця інтерпретація розглядає ймовірність як відносну частоту "в довгостроковій перспективі" результатів. Модифікацією цього є ймовірність схильності , яка інтерпретує ймовірність як тенденцію деякого експерименту давати певний результат, навіть якщо він виконується лише один раз.
Суб'єктивісти призначають числа за суб'єктивною ймовірністю, тобто як ступінь віри. [5] Ступінь віри тлумачиться як «ціна, за якою ви б купили або продали ставку, яка сплачує 1 одиницю корисності, якщо E, 0, якщо не E», [6] хоча це тлумачення не є загальновизнаним. [7] Найпопулярнішою версією суб’єктивної ймовірності є байєсівська ймовірність , яка включає експертні знання, а також експериментальні дані для створення ймовірностей. Експертні знання представлені деяким (суб'єктивним) попереднім розподілом ймовірностей . Ці дані включені у функцію ймовірності . Добуток апріора та правдоподібності при нормалізації призводить до апостеріорного розподілу ймовірностей , який включає всю інформацію, відому на сьогоднішній день. [8] Згідно з теоремою згоди Аумана , байєсівські агенти, чиї попередні переконання подібні, в кінцевому підсумку отримають подібні постериорні переконання. Однак достатньо різні попередні можуть призвести до різних висновків, незалежно від того, скільки інформації поділяють агенти. [9]
Слово ймовірність походить від латинського probabilitas, що також може означати «чесність», міру авторитету свідка в судовій справі в Європі , і часто співвідноситься зі шляхетністю свідка . У певному сенсі це значно відрізняється від сучасного значення ймовірності , яке, навпаки, є мірою ваги емпіричних доказів і виходить на основі індуктивних міркувань і статистичних висновків.

Наукове вивчення ймовірностей є сучасним розвитком математики. Азартні ігри показують, що в історії був інтерес до кількісного визначення ідей ймовірності, але точні математичні описи виникли набагато пізніше. Є причини повільного розвитку математики ймовірностей. Тоді як азартні ігри стали поштовхом для математичного вивчення ймовірності, фундаментальні питання все ще затьмарені забобонами.

За словами Річарда Джефрі , «до середини сімнадцятого століття термін «ймовірний» (лат. probabilis ) означав « схвалений » і застосовувався в цьому сенсі, однозначно, до думки та дії. Ймовірна дія або думка була такою, як розумні люди взялися б або трималися, за цих обставин». Однак, особливо в юридичному контексті, «ймовірний» також може застосовуватися до пропозицій, для яких є вагомі докази.

Джероламо Кардано (16 століття)

Крістіан Гюйгенс опублікував одну з перших книг про ймовірність (17 століття).
Італійський ерудит шістнадцятого століття Джероламо Кардано продемонстрував ефективність визначення шансів як співвідношення сприятливих результатів до несприятливих (що означає, що ймовірність події визначається співвідношенням сприятливих результатів до загальної кількості можливих результатів [14] ) . Окрім елементарної роботи Кардано, вчення про ймовірності бере початок від листування П’єра де Ферма та Блеза Паскаля (1654). Крістіан Гюйгенс (1657) дав найпершу з відомих наукових розробок цього питання. [15] Ars Conjectandi Якоба Бернуллі (посмертно, 1713) і Доктрина випадковостей Авраама де Муавра (1718) розглядали цей предмет як розділ математики. [16] Історію раннього розвитку самого поняття математичної ймовірності див. у книгах Яна Хекінга « Виникнення ймовірності» [10] і Джеймса Франкліна «Наука про припущення» [17] .

Теорію помилок можна простежити до Opera Miscellanea Роджера Котса (посмертно, 1722), але мемуари, підготовлені Томасом Сімпсоном у 1755 році (надруковані 1756), вперше застосували цю теорію до обговорення помилок спостереження. [18] Перевидання (1757) цих мемуарів викладає аксіоми про те, що позитивні та негативні помилки однаково вірогідні, і що певні межі, які можна призначити, визначають діапазон усіх помилок. Сімпсон також обговорює безперервні помилки та описує криву ймовірності.

Перші два закони помилки, які були запропоновані, походять від П’єра-Симона Лапласа . Перший закон був опублікований у 1774 році, і в ньому говорилося, що частоту помилки можна виразити як експоненціальну функцію числової величини помилки без урахування знака. Другий закон похибки був запропонований у 1778 році Лапласом і стверджував, що частота похибки є експоненціальною функцією квадрата похибки. [19] Другий закон похибки називається нормальним розподілом або законом Гаусса. «Історично важко приписати цей закон Гауссу, який, незважаючи на його добре відому передчасність розвитку, ймовірно, не зробив цього відкриття, перш ніж йому виповнилося два роки». [19]

Даніель Бернуллі (1778) запровадив принцип максимального добутку ймовірностей системи одночасних помилок.


Карл Фрідріх Гаус
Адрієн-Марі Лежандр (1805) розробив метод найменших квадратів і представив його у своїй праці Nouvelles méthodes pour la détermination des orbites des comètes ( Нові методи визначення орбіт комет ). [20] Не знаючи про внесок Лежандра, ірландсько-американський письменник Роберт Едрейн , редактор «Аналітика» (1808), вперше вивів закон легкої помилки,

ϕ
(
x
)
=
в
д
−
ч
2
x
2
,
{\displaystyle \phi (x)=ce^{-h^{2}x^{2}},}
де
ч
 {\displaystyle h}є константою, що залежить від точності спостереження, і
в
 {\displaystyle c}є масштабним коефіцієнтом, який гарантує, що площа під кривою дорівнює 1. Він навів два докази, другий, по суті, такий самий, як Джон Гершель (1850). [ потрібне цитування ] Гаусс надав перший доказ, який, здається, був відомий у Європі (третій після Адрена) у 1809 році. Подальші докази надали Лаплас (1810, 1812), Гаусс (1823), Джеймс Айворі (1825, 1826). , Хаген (1837), Фрідріх Бессель (1838), В. Ф. Донкін (1844, 1856) і Морган Крофтон (1870). Іншими авторами були Елліс (1844), Де Морган (1864), Глейшер (1872) і Джованні Скіапареллі (1875). Формула Петерса (1856) [ потрібне уточнення ] для r , ймовірної помилки одного спостереження, добре відома.

У дев’ятнадцятому столітті серед авторів загальної теорії були Лаплас , Сильвестр Лакруа (1816), Літроу (1833), Адольф Кетле (1853), Ріхард Дедекінд (1860), Гельмерт (1872), Герман Лоран (1873), Ліагре, Дідіон і Карл Пірсон . Август Де Морган і Джордж Буль вдосконалили виклад теорії.

У 1906 році Андрій Марков ввів [21] поняття ланцюгів Маркова , яке відіграло важливу роль у теорії стохастичних процесів та її застосуваннях. Сучасна теорія ймовірності, заснована на теорії міри, була розроблена Андрієм Колмогоровим у 1931 році. [22]

Щодо геометричної сторони, дописувачі The Educational Times включали Міллера, Крофтона, Макколла, Волстенхолма, Вотсона та Артемаса Мартіна . [23] Для отримання додаткової інформації дивіться інтегральну геометрію .

Як і інші теорії , теорія ймовірності є представленням своїх концепцій у формальних термінах, тобто в термінах, які можна розглядати окремо від їх значення. Ці формальні терміни маніпулюються правилами математики та логіки, і будь-які результати інтерпретуються або перекладаються назад у область проблеми.

Було щонайменше дві успішні спроби формалізувати ймовірність, а саме формулювання Колмогорова та формулювання Кокса . У формулюванні Колмогорова (див. також ймовірнісний простір ), множини інтерпретуються як події , а ймовірність як міра на класі множин. У теоремі Кокса ймовірність береться як примітив (тобто далі не аналізується), а акцент робиться на побудові послідовного призначення значень ймовірності пропозиціям. В обох випадках закони ймовірності однакові, за винятком технічних деталей.

Існують інші методи кількісного визначення невизначеності, такі як теорія Демпстера–Шейфера або теорія можливостей , але вони суттєво відрізняються і несумісні із зазвичай розуміними законами ймовірності.

Теорія ймовірностей застосовується в повсякденному житті в оцінці та моделюванні ризиків . Страхова галузь і ринки використовують актуарну науку для визначення цін і прийняття торгових рішень. Уряди застосовують імовірнісні методи в екологічному регулюванні , аналізі прав і фінансовому регулюванні .

Прикладом використання теорії ймовірностей у торгівлі акціями є вплив уявної ймовірності будь-якого широкопоширеного конфлікту на Близькому Сході на ціни на нафту, що має хвильовий вплив на економіку в цілому. Оцінка товарним трейдером того, що війна більш імовірна, може підвищити або знизити ціни на цей товар і сигналізувати про таку думку іншим трейдерам. Відповідно, ймовірності не оцінюються ні незалежно, ні обов’язково раціонально. Теорія поведінкових фінансів виникла, щоб описати вплив такого групового мислення на ціноутворення, політику, а також на мир і конфлікти. [24]

Окрім фінансової оцінки, ймовірність може використовуватися для аналізу тенденцій у біології (наприклад, поширення хвороби), а також екології (наприклад, біологічні квадрати Пуннетта ). [25] Як і у випадку з фінансами, оцінка ризику може бути використана як статистичний інструмент для розрахунку ймовірності виникнення небажаних подій і може допомогти у впровадженні протоколів, щоб уникнути зустрічі з такими обставинами. Імовірність використовується для розробки азартних ігор , щоб казино могли отримувати гарантований прибуток, але забезпечували виплати гравцям, які є досить частими, щоб стимулювати продовження гри. [26]

Іншим важливим застосуванням теорії ймовірностей у повсякденному житті є надійність. Багато споживчих товарів, таких як автомобілі та побутова електроніка, використовують теорію надійності при проектуванні продукту, щоб зменшити ймовірність відмови. Імовірність несправності може вплинути на рішення виробника щодо гарантії на продукт . [27]

Модель мови кешу та інші статистичні моделі мови , які використовуються в обробці природної мови, також є прикладами застосування теорії ймовірностей.

Розглянемо експеримент, який може дати кілька результатів. Сукупність усіх можливих результатів називається простором вибірки експерименту, іноді позначається як
Ω
 {\displaystyle \Omega }. Набір потужностей простору вибірки формується шляхом розгляду всіх різних наборів можливих результатів. Наприклад, кидання кубика може дати шість можливих результатів. Одна сукупність можливих результатів дає непарне число на кубику. Таким чином, підмножина {1,3,5} є елементом набору потужностей вибіркового простору кидків кубиків. Ці колекції називаються «подіями». У цьому випадку {1,3,5} — це подія, коли кубик випаде на якесь непарне число. Якщо результати, які фактично відбуваються, потрапляють у певну подію, кажуть, що подія відбулася.

Імовірність — це спосіб присвоєння кожній події значення від нуля до одиниці з вимогою, щоб подія, що складається з усіх можливих результатів (у нашому прикладі подія {1,2,3,4,5,6}), була присвоєно значення одиниці. Щоб кваліфікувати як ймовірність, призначення значень має задовольняти вимозі, що для будь-якої сукупності взаємовиключних подій (подій без спільних результатів, таких як події {1,6}, {3} і {2,4}) , ймовірність того, що принаймні одна з подій відбудеться, визначається сумою ймовірностей усіх окремих подій. [28]

Це математичне визначення ймовірності може поширюватися на нескінченні простори вибірок і навіть незліченні простори вибірок, використовуючи концепцію міри.

Якщо дві події A і B відбуваються під час одного виконання експерименту, це називається перетином або спільною ймовірністю A і B , що позначається як
П
(
А
∩
Б
)
.
 {\displaystyle P(A\cap B).}

Якщо дві події, A і B , незалежні, то спільна ймовірність

П
(
А
 і
Б
)
=
П
(
А
∩
Б
)
=
П
(
А
)
П
(
Б
)
.
{\displaystyle P(A{\mbox{ and }}B)=P(A\cap B)=P(A)P(B).}

Події A і B зображені як незалежні та незалежні в просторі Ω.
Наприклад, якщо підкинуто дві монети, то шанс, що обидві будуть головами, дорівнює
1
2
×
1
2
=
1
4
.
 {\displaystyle {\tfrac {1}{2}}\times {\tfrac {1}{2}}={\tfrac {1}{4}}.}[32]

Якщо будь-яка подія A або подія B можуть відбутися, але ніколи обидві одночасно, тоді вони називаються взаємовиключними подіями.

Якщо дві події є взаємовиключними , то ймовірність того, що вони відбудуться, позначається як
П
(
А
∩
Б
)
 {\displaystyle P(A\cap B)}і

П
(
А
 і
Б
)
=
П
(
А
∩
Б
)
=
0
{\displaystyle P(A{\mbox{ and }}B)=P(A\cap B)=0}Якщо дві події є взаємовиключними , то ймовірність кожної з них позначається як
П
(
А
∪
Б
)
 {\displaystyle P(A\cup B)}і
П
(
А
 або
Б
)
=
П
(
А
∪
Б
)
=
П
(
А
)
+
П
(
Б
)
−
П
(
А
∩
Б
)
=
П
(
А
)
+
П
(
Б
)
−
0
=
П
(
А
)
+
П
(
Б
)
{\displaystyle P(A{\mbox{ or }}B)=P(A\cup B)=P(A)+P(B)-P(A\cap B)=P(A)+P(B)-0=P(A)+P(B)}Наприклад, шанс викинути 1 або 2 на шестигранному кубику становить
П
(
1
 або
2
)
=
П
(
1
)
+
П
(
2
)
=
1
6
+
1
6
=
1
3
.
 {\displaystyle P(1{\mbox{ or }}2)=P(1)+P(2)={\tfrac {1}{6}}+{\tfrac {1}{6}}={\tfrac {1}{3}}.}
Не (обов’язково) взаємовиключні події
Якщо події не є (обов’язково) взаємовиключними, тоді

П
(
А
 або
Б
)
=
П
(
А
∪
Б
)
=
П
(
А
)
+
П
(
Б
)
−
П
(
А
 і
Б
)
.
{\displaystyle P\left(A{\hbox{ or }}B\right)=P(A\cup B)=P\left(A\right)+P\left(B\right)-P\left(A{\mbox{ and }}B\right).}Переписаний,
П
(
А
∪
Б
)
=
П
(
А
)
+
П
(
Б
)
−
П
(
А
∩
Б
)
{\displaystyle P\left(A\cup B\right)=P\left(A\right)+P\left(B\right)-P\left(A\cap B\right)}Наприклад, коли витягуєте карту з колоди карт, шанс отримати серце або карту з обличчям (J, Q, K) (або обидві) становить
13
52
+
12
52
−
3
52
=
11
26
,
 {\displaystyle {\tfrac {13}{52}}+{\tfrac {12}{52}}-{\tfrac {3}{52}}={\tfrac {11}{26}},}оскільки серед 52 карт колоди 13 є червами, 12 є картами з обличчям, а 3 є обома: тут можливості, включені в «3, які є обома», включені в кожну з «13 карт із сердечками» та «12 обличчями». карток», але їх слід зараховувати лише один раз.
Це можна розширити для кількох не (обов’язково) взаємовиключних подій. Для трьох подій це відбувається таким чином:

П
(
А
∪
Б
∪
C
)
=
П
(
(
А
∪
Б
)
∪
C
)
=
П
(
А
∪
Б
)
+
П
(
C
)
−
П
(
(
А
∪
Б
)
∩
C
)
=
П
(
А
)
+
П
(
Б
)
−
П
(
А
∩
Б
)
+
П
(
C
)
−
П
(
(
А
∩
C
)
∪
(
Б
∩
C
)
)
=
П
(
А
)
+
П
(
Б
)
+
П
(
C
)
−
П
(
А
∩
Б
)
−
(
П
(
А
∩
C
)
+
П
(
Б
∩
C
)
−
П
(
(
А
∩
C
)
∩
(
Б
∩
C
)
)
)
П
(
А
∪
Б
∪
C
)
=
П
(
А
)
+
П
(
Б
)
+
П
(
C
)
−
П
(
А
∩
Б
)
−
П
(
А
∩
C
)
−
П
(
Б
∩
C
)
+
П
(
А
∩
Б
∩
C
)
{\displaystyle {\begin{aligned}P\left(A\cup B\cup C\right)=&P\left(\left(A\cup B\right)\cup C\right)\\=&P\left(A\cup B\right)+P\left(C\right)-P\left(\left(A\cup B\right)\cap C\right)\\=&P\left(A\right)+P\left(B\right)-P\left(A\cap B\right)+P\left(C\right)-P\left(\left(A\cap C\right)\cup \left(B\cap C\right)\right)\\=&P\left(A\right)+P\left(B\right)+P\left(C\right)-P\left(A\cap B\right)-\left(P\left(A\cap C\right)+P\left(B\cap C\right)-P\left(\left(A\cap C\right)\cap \left(B\cap C\right)\right)\right)\\P\left(A\cup B\cup C\right)=&P\left(A\right)+P\left(B\right)+P\left(C\right)-P\left(A\cap B\right)-P\left(A\cap C\right)-P\left(B\cap C\right)+P\left(A\cap B\cap C\right)\end{aligned}}}Отже, можна побачити, що цей шаблон може повторюватися для будь-якої кількості подій.
Умовна ймовірність — це ймовірність деякої події A за умови настання якоїсь іншої події B.

У детерміністичному всесвіті, заснованому на ньютонівських концепціях, не було б імовірності, якби всі умови були відомі ( демон Лапласа ) (але існують ситуації, в яких чутливість до початкових умов перевищує нашу здатність їх вимірювати, тобто знати). У випадку колеса рулетки , якщо сила руки та період цієї сили відомі, число, на якому зупиниться кулька, було б впевненим (хоча з практичної точки зору це, ймовірно, справедливо лише для колесо рулетки, яке не було точно вирівняно – як показало Newtonian Casino Томаса А. Басса ). Це також передбачає знання інерції та тертя колеса, ваги, гладкості та округлості м’яча, зміни швидкості руки під час повороту тощо. Таким чином, імовірнісний опис може бути більш корисним, ніж механіка Ньютона, для аналізу моделі результатів повторюваних обертів колеса рулетки. Фізики стикаються з такою ж ситуацією в кінетичній теорії газів , де система, хоч і детермінована в принципі , є настільки складною (з кількістю молекул, як правило, порядку величини сталої Авогадро 6,02 × 10 23 ), що можливий лише статистичний опис його властивостей.

Для опису квантових явищ потрібна теорія ймовірностей. Революційним відкриттям фізики початку 20-го століття став випадковий характер усіх фізичних процесів, які відбуваються в субатомних масштабах і керуються законами квантової механіки . Цільова хвильова функція розвивається детерміновано, але, згідно з Копенгагенською інтерпретацією , вона має справу з ймовірностями спостереження, результат пояснюється колапсом хвильової функції під час спостереження. Проте втрата детермінізму на догоду інструменталізму не зустріла загального схвалення. Альберт Ейнштейн у своєму листі до Макса Борна сказав : «Я переконаний, що Бог не грає в кості». [37] Як і Ейнштейн, Ервін Шредінгер , який відкрив хвильову функцію, вважав, що квантова механіка є статистичною апроксимацією детермінованої реальності , що лежить в її основі . [38] У деяких сучасних інтерпретаціях статистичної механіки вимірювання квантова декогеренція використовується для пояснення появи суб’єктивно імовірнісних експериментальних результатів.